{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TPC-H Dataset Generator — Pure PySpark (Serverless Compatible)\n",
        "\n",
        "Generates the full **TPC-H** benchmark dataset **entirely in PySpark** — no shell commands, no `dbgen` binary, no access to `dbfs:/tmp/` required.\n",
        "\n",
        "| Table | SF-1 Rows | SF-100 Rows |\n",
        "|-------|-----------|-------------|\n",
        "| REGION | 5 | 5 |\n",
        "| NATION | 25 | 25 |\n",
        "| SUPPLIER | 10,000 | 1,000,000 |\n",
        "| PART | 200,000 | 20,000,000 |\n",
        "| PARTSUPP | 800,000 | 80,000,000 |\n",
        "| CUSTOMER | 150,000 | 15,000,000 |\n",
        "| ORDERS | 1,500,000 | 150,000,000 |\n",
        "| LINEITEM | ~6,000,000 | ~600,000,000 |\n",
        "\n",
        "**Storage**: Unity Catalog managed Delta tables (no external paths).\n",
        "\n",
        "Start with `SCALE_FACTOR = 1` to validate, then bump to 10 or 100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1 — Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Configuration ─────────────────────────────────────────────────────────────\n",
        "# Uses whatever catalog is currently set on the cluster / session.\n",
        "# SCALE_FACTOR controls data volume:  1 ≈ 1 GB, 10 ≈ 10 GB, 100 ≈ 100 GB.\n",
        "# To generate 100 GB, just change SCALE_FACTOR to 100 and re-run all cells.\n",
        "\n",
        "CATALOG      = spark.catalog.currentCatalog()   # auto-detect current catalog\n",
        "SCHEMA       = \"tpch\"                            # schema that will hold the tables\n",
        "SCALE_FACTOR = 1                                 # 1 GB to start; change to 100 for full scale\n",
        "\n",
        "# Derived row counts per TPC-H spec\n",
        "SF = SCALE_FACTOR\n",
        "N_SUPPLIER  = 10_000   * SF\n",
        "N_PART      = 200_000  * SF\n",
        "N_PARTSUPP  = 800_000  * SF   # 4 records per part\n",
        "N_CUSTOMER  = 150_000  * SF\n",
        "N_ORDERS    = 1_500_000 * SF\n",
        "# LINEITEM is ~4x ORDERS on average (generated per-order below)\n",
        "\n",
        "full_schema = f\"{CATALOG}.{SCHEMA}\"\n",
        "\n",
        "print(f\"Current catalog: {CATALOG}\")\n",
        "print(f\"Target schema:   {full_schema}  |  Scale Factor: {SF}\")\n",
        "print(f\"Expected rows — SUPPLIER: {N_SUPPLIER:,}  PART: {N_PART:,}  CUSTOMER: {N_CUSTOMER:,}  ORDERS: {N_ORDERS:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2 — Create Catalog & Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "369a8b7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Catalog already exists (we detected it via currentCatalog), so just ensure the schema.\n",
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {full_schema}\")\n",
        "spark.sql(f\"USE {full_schema}\")\n",
        "print(f\"Using {full_schema}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3 — Imports & Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import (\n",
        "    StructType, StructField,\n",
        "    IntegerType, LongType, StringType, DoubleType, DateType,\n",
        ")\n",
        "\n",
        "# Deterministic random helper (seed-based for reproducibility)\n",
        "def rand_int(seed, low, high):\n",
        "    \"\"\"Return a Column expression: random int in [low, high).\"\"\"\n",
        "    return (F.rand(seed) * (high - low) + low).cast(IntegerType())\n",
        "\n",
        "def rand_double(seed, low, high, decimals=2):\n",
        "    \"\"\"Return a Column expression: random double in [low, high), rounded.\"\"\"\n",
        "    return F.round(F.rand(seed) * (high - low) + low, decimals)\n",
        "\n",
        "def pick_one(seed, arr_expr):\n",
        "    \"\"\"Pick a random element from a SQL array literal.\"\"\"\n",
        "    return F.expr(f\"element_at({arr_expr}, int(rand({seed}) * size({arr_expr})) + 1)\")\n",
        "\n",
        "def write_table(df, table_name):\n",
        "    \"\"\"Write a DataFrame as a managed Delta table (overwrite).\"\"\"\n",
        "    fqn = f\"{full_schema}.{table_name}\"\n",
        "    df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(fqn)\n",
        "    cnt = spark.table(fqn).count()\n",
        "    print(f\"  ✓ {fqn} — {cnt:,} rows\")\n",
        "    return cnt\n",
        "\n",
        "print(\"Helpers ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4 — REGION (5 rows, static)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "region_data = [\n",
        "    (0, \"AFRICA\",      \"Special offers for African customers.\"),\n",
        "    (1, \"AMERICA\",     \"Customers from the Americas region.\"),\n",
        "    (2, \"ASIA\",        \"Customers from the Asia-Pacific region.\"),\n",
        "    (3, \"EUROPE\",      \"Customers from the European region.\"),\n",
        "    (4, \"MIDDLE EAST\", \"Customers from the Middle East region.\"),\n",
        "]\n",
        "\n",
        "region_schema = StructType([\n",
        "    StructField(\"r_regionkey\", IntegerType(), False),\n",
        "    StructField(\"r_name\",      StringType(), False),\n",
        "    StructField(\"r_comment\",   StringType(), True),\n",
        "])\n",
        "\n",
        "df_region = spark.createDataFrame(region_data, schema=region_schema)\n",
        "write_table(df_region, \"region\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5 — NATION (25 rows, static)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nation_data = [\n",
        "    ( 0, \"ALGERIA\",        0), ( 1, \"ARGENTINA\",      1), ( 2, \"BRAZIL\",         1),\n",
        "    ( 3, \"CANADA\",         1), ( 4, \"EGYPT\",          4), ( 5, \"ETHIOPIA\",       0),\n",
        "    ( 6, \"FRANCE\",         3), ( 7, \"GERMANY\",        3), ( 8, \"INDIA\",          2),\n",
        "    ( 9, \"INDONESIA\",      2), (10, \"IRAN\",           4), (11, \"IRAQ\",           4),\n",
        "    (12, \"JAPAN\",          2), (13, \"JORDAN\",         4), (14, \"KENYA\",          0),\n",
        "    (15, \"MOROCCO\",        0), (16, \"MOZAMBIQUE\",     0), (17, \"PERU\",           1),\n",
        "    (18, \"CHINA\",          2), (19, \"ROMANIA\",        3), (20, \"SAUDI ARABIA\",   4),\n",
        "    (21, \"VIETNAM\",        2), (22, \"RUSSIA\",         3), (23, \"UNITED KINGDOM\", 3),\n",
        "    (24, \"UNITED STATES\",  1),\n",
        "]\n",
        "\n",
        "nation_schema = StructType([\n",
        "    StructField(\"n_nationkey\", IntegerType(), False),\n",
        "    StructField(\"n_name\",      StringType(), False),\n",
        "    StructField(\"n_regionkey\", IntegerType(), False),\n",
        "])\n",
        "\n",
        "df_nation = spark.createDataFrame(nation_data, schema=nation_schema)\n",
        "df_nation = df_nation.withColumn(\"n_comment\", F.concat(F.lit(\"Nation comment for \"), F.col(\"n_name\")))\n",
        "write_table(df_nation, \"nation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6 — SUPPLIER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_supplier = (\n",
        "    spark.range(1, N_SUPPLIER + 1)\n",
        "    .withColumnRenamed(\"id\", \"s_suppkey\")\n",
        "    .withColumn(\"s_name\",      F.concat(F.lit(\"Supplier#\"), F.lpad(F.col(\"s_suppkey\").cast(\"string\"), 9, \"0\")))\n",
        "    .withColumn(\"s_address\",   F.concat(F.lit(\"Addr-\"), (F.rand(101) * 99999).cast(IntegerType()).cast(\"string\")))\n",
        "    .withColumn(\"s_nationkey\", rand_int(103, 0, 25))\n",
        "    .withColumn(\"s_phone\",     F.concat(\n",
        "                                    (F.col(\"s_nationkey\") + 10).cast(\"string\"), F.lit(\"-\"),\n",
        "                                    F.lpad((F.rand(107) * 999).cast(IntegerType()).cast(\"string\"), 3, \"0\"), F.lit(\"-\"),\n",
        "                                    F.lpad((F.rand(109) * 999).cast(IntegerType()).cast(\"string\"), 3, \"0\"), F.lit(\"-\"),\n",
        "                                    F.lpad((F.rand(113) * 9999).cast(IntegerType()).cast(\"string\"), 4, \"0\"),\n",
        "                                ))\n",
        "    .withColumn(\"s_acctbal\",   rand_double(127, -999.99, 9999.99))\n",
        "    .withColumn(\"s_comment\",   F.concat(F.lit(\"Supplier comment \"), F.col(\"s_suppkey\").cast(\"string\")))\n",
        ")\n",
        "\n",
        "write_table(df_supplier, \"supplier\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7 — PART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TPC-H part type components\n",
        "_types  = \"array('STANDARD','SMALL','MEDIUM','LARGE','ECONOMY','PROMO')\"\n",
        "_metals = \"array('TIN','NICKEL','BRASS','STEEL','COPPER')\"\n",
        "_finish = \"array('POLISHED','BURNISHED','PLATED','ANODIZED','BRUSHED')\"\n",
        "_containers = \"array('SM CASE','SM BOX','SM PACK','SM PKG','SM JAR','SM BAG','SM CAN','SM DRUM','MED CASE','MED BOX','MED PACK','MED PKG','MED JAR','MED BAG','MED CAN','MED DRUM','LG CASE','LG BOX','LG PACK','LG PKG','LG JAR','LG BAG','LG CAN','LG DRUM','WRAP CASE','WRAP BOX','WRAP PACK','WRAP PKG','WRAP JAR','WRAP BAG','WRAP CAN','WRAP DRUM','JUMBO CASE','JUMBO BOX','JUMBO PACK','JUMBO PKG','JUMBO JAR','JUMBO BAG','JUMBO CAN','JUMBO DRUM')\"\n",
        "_brands = \"array('Brand#11','Brand#12','Brand#13','Brand#14','Brand#15','Brand#21','Brand#22','Brand#23','Brand#24','Brand#25','Brand#31','Brand#32','Brand#33','Brand#34','Brand#35','Brand#41','Brand#42','Brand#43','Brand#44','Brand#45','Brand#51','Brand#52','Brand#53','Brand#54','Brand#55')\"\n",
        "_colors = \"array('almond','antique','aquamarine','azure','beige','bisque','black','blanched','blue','blush','brown','burlywood','burnished','chartreuse','chiffon','chocolate','coral','cornflower','cornsilk','cream','cyan','dark','deep','dim','dodger','drab','firebrick','floral','forest','frosted','gainsboro','ghost','goldenrod','green','grey','honeydew','hot','indian','ivory','khaki','lace','lavender','lawn','lemon','light','lime','linen','magenta','maroon','medium','metallic','midnight','mint','misty','moccasin','navajo','navy','olive','orange','orchid','pale','papaya','peach','peru','pink','plum','powder','puff','purple','red','rosy','royal','saddle','salmon','sandy','seashell','sienna','sky','slate','smoke','snow','spring','steel','tan','thistle','tomato','turquoise','violet','wheat','white','yellow')\"\n",
        "\n",
        "df_part = (\n",
        "    spark.range(1, N_PART + 1)\n",
        "    .withColumnRenamed(\"id\", \"p_partkey\")\n",
        "    .withColumn(\"p_name\", F.concat_ws(\" \",\n",
        "                    pick_one(201, _colors), pick_one(202, _colors),\n",
        "                    pick_one(203, _colors), pick_one(204, _colors),\n",
        "                    pick_one(205, _colors)))\n",
        "    .withColumn(\"p_mfgr\",        F.concat(F.lit(\"Manufacturer#\"), (rand_int(211, 1, 6)).cast(\"string\")))\n",
        "    .withColumn(\"p_brand\",       pick_one(213, _brands))\n",
        "    .withColumn(\"p_type\",        F.concat_ws(\" \", pick_one(215, _types), pick_one(217, _metals), pick_one(219, _finish)))\n",
        "    .withColumn(\"p_size\",        rand_int(221, 1, 51))\n",
        "    .withColumn(\"p_container\",   pick_one(223, _containers))\n",
        "    .withColumn(\"p_retailprice\", F.round(\n",
        "                                    (900.0 + (F.col(\"p_partkey\") / 10) % F.lit(200.01)\n",
        "                                     + (F.col(\"p_partkey\") % F.lit(1000)) * F.lit(0.01)), 2))\n",
        "    .withColumn(\"p_comment\",     F.concat(F.lit(\"Part comment \"), F.col(\"p_partkey\").cast(\"string\")))\n",
        ")\n",
        "\n",
        "write_table(df_part, \"part\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8 — PARTSUPP (4 suppliers per part)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Each part has 4 suppliers → cross with array(0,1,2,3)\n",
        "df_partsupp = (\n",
        "    spark.range(1, N_PART + 1)\n",
        "    .withColumnRenamed(\"id\", \"ps_partkey\")\n",
        "    .withColumn(\"_idx\", F.explode(F.array(*[F.lit(i) for i in range(4)])))\n",
        "    .withColumn(\"ps_suppkey\",    ((F.col(\"ps_partkey\") + F.col(\"_idx\") * (N_PART // 4 + 1)) % F.lit(N_SUPPLIER) + 1).cast(IntegerType()))\n",
        "    .withColumn(\"ps_availqty\",   rand_int(301, 1, 10000))\n",
        "    .withColumn(\"ps_supplycost\", rand_double(303, 1.0, 1000.0))\n",
        "    .withColumn(\"ps_comment\",    F.concat(F.lit(\"PS comment \"), F.col(\"ps_partkey\").cast(\"string\"), F.lit(\"-\"), F.col(\"_idx\").cast(\"string\")))\n",
        "    .drop(\"_idx\")\n",
        ")\n",
        "\n",
        "write_table(df_partsupp, \"partsupp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 9 — CUSTOMER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_segments = \"array('AUTOMOBILE','BUILDING','FURNITURE','HOUSEHOLD','MACHINERY')\"\n",
        "\n",
        "df_customer = (\n",
        "    spark.range(1, N_CUSTOMER + 1)\n",
        "    .withColumnRenamed(\"id\", \"c_custkey\")\n",
        "    .withColumn(\"c_name\",       F.concat(F.lit(\"Customer#\"), F.lpad(F.col(\"c_custkey\").cast(\"string\"), 9, \"0\")))\n",
        "    .withColumn(\"c_address\",    F.concat(F.lit(\"CustAddr-\"), (F.rand(401) * 99999).cast(IntegerType()).cast(\"string\")))\n",
        "    .withColumn(\"c_nationkey\",  rand_int(403, 0, 25))\n",
        "    .withColumn(\"c_phone\",      F.concat(\n",
        "                                    (F.col(\"c_nationkey\") + 10).cast(\"string\"), F.lit(\"-\"),\n",
        "                                    F.lpad((F.rand(407) * 999).cast(IntegerType()).cast(\"string\"), 3, \"0\"), F.lit(\"-\"),\n",
        "                                    F.lpad((F.rand(409) * 999).cast(IntegerType()).cast(\"string\"), 3, \"0\"), F.lit(\"-\"),\n",
        "                                    F.lpad((F.rand(411) * 9999).cast(IntegerType()).cast(\"string\"), 4, \"0\"),\n",
        "                                ))\n",
        "    .withColumn(\"c_acctbal\",    rand_double(421, -999.99, 9999.99))\n",
        "    .withColumn(\"c_mktsegment\", pick_one(431, _segments))\n",
        "    .withColumn(\"c_comment\",    F.concat(F.lit(\"Customer comment \"), F.col(\"c_custkey\").cast(\"string\")))\n",
        ")\n",
        "\n",
        "write_table(df_customer, \"customer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 10 — ORDERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_priorities = \"array('1-URGENT','2-HIGH','3-MEDIUM','4-NOT SPECIFIED','5-LOW')\"\n",
        "\n",
        "# TPC-H order dates span 1992-01-01 to 1998-08-02 (~2405 days)\n",
        "_order_start = F.to_date(F.lit(\"1992-01-01\"))\n",
        "_order_days  = 2405\n",
        "\n",
        "df_orders = (\n",
        "    spark.range(1, N_ORDERS + 1)\n",
        "    .withColumnRenamed(\"id\", \"o_orderkey\")\n",
        "    .withColumn(\"o_custkey\",       (F.rand(501) * N_CUSTOMER).cast(IntegerType()) + 1)\n",
        "    .withColumn(\"o_orderstatus\",   F.expr(\"element_at(array('O','F','P'), int(rand(503)*3)+1)\"))\n",
        "    .withColumn(\"o_totalprice\",    rand_double(505, 900.0, 550000.0))\n",
        "    .withColumn(\"o_orderdate\",     F.date_add(_order_start, rand_int(507, 0, _order_days)))\n",
        "    .withColumn(\"o_orderpriority\", pick_one(509, _priorities))\n",
        "    .withColumn(\"o_clerk\",         F.concat(F.lit(\"Clerk#\"), F.lpad((F.rand(511) * (N_SUPPLIER / 10)).cast(IntegerType()).cast(\"string\"), 9, \"0\")))\n",
        "    .withColumn(\"o_shippriority\",  F.lit(0))\n",
        "    .withColumn(\"o_comment\",       F.concat(F.lit(\"Order comment \"), F.col(\"o_orderkey\").cast(\"string\")))\n",
        ")\n",
        "\n",
        "write_table(df_orders, \"orders\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 11 — LINEITEM (largest table, ~4× ORDERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Each order gets between 1 and 7 line items (avg ~4).\n",
        "# We generate by exploding a random line-count per order.\n",
        "\n",
        "_shipmodes  = \"array('REG AIR','AIR','RAIL','SHIP','TRUCK','MAIL','FOB')\"\n",
        "_instructs  = \"array('DELIVER IN PERSON','COLLECT COD','NONE','TAKE BACK RETURN')\"\n",
        "_rflag      = \"array('R','A','N')\"\n",
        "_lstatus    = \"array('O','F')\"\n",
        "\n",
        "# Start from orders to inherit o_orderkey and o_orderdate\n",
        "df_lineitem = (\n",
        "    spark.table(f\"{full_schema}.orders\")\n",
        "    .select(\"o_orderkey\", \"o_orderdate\")\n",
        "    # Random number of line items per order (1 to 7)\n",
        "    .withColumn(\"_num_lines\", rand_int(601, 1, 8))\n",
        "    .withColumn(\"l_linenumber\", F.explode(F.sequence(F.lit(1), F.col(\"_num_lines\"))))\n",
        "    .drop(\"_num_lines\")\n",
        "    .withColumnRenamed(\"o_orderkey\", \"l_orderkey\")\n",
        "    # Foreign keys\n",
        "    .withColumn(\"l_partkey\",       (F.rand(603) * N_PART).cast(IntegerType()) + 1)\n",
        "    .withColumn(\"l_suppkey\",       (F.rand(605) * N_SUPPLIER).cast(IntegerType()) + 1)\n",
        "    # Quantities and pricing\n",
        "    .withColumn(\"l_quantity\",      rand_int(607, 1, 51).cast(DoubleType()))\n",
        "    .withColumn(\"l_extendedprice\", F.round(F.col(\"l_quantity\") * rand_double(609, 1.0, 1000.0), 2))\n",
        "    .withColumn(\"l_discount\",      F.round(F.rand(611) * 0.10, 2))\n",
        "    .withColumn(\"l_tax\",           F.round(F.rand(613) * 0.08, 2))\n",
        "    # Status flags\n",
        "    .withColumn(\"l_returnflag\",    pick_one(615, _rflag))\n",
        "    .withColumn(\"l_linestatus\",    pick_one(617, _lstatus))\n",
        "    # Dates: shipdate 1-121 days after orderdate, commitdate 30-90, receiptdate 1-30 after ship\n",
        "    .withColumn(\"l_shipdate\",      F.date_add(F.col(\"o_orderdate\"), rand_int(619, 1, 122)))\n",
        "    .withColumn(\"l_commitdate\",    F.date_add(F.col(\"o_orderdate\"), rand_int(621, 30, 91)))\n",
        "    .withColumn(\"l_receiptdate\",   F.date_add(F.col(\"l_shipdate\"),  rand_int(623, 1, 31)))\n",
        "    .withColumn(\"l_shipinstruct\",  pick_one(625, _instructs))\n",
        "    .withColumn(\"l_shipmode\",      pick_one(627, _shipmodes))\n",
        "    .withColumn(\"l_comment\",       F.concat(F.lit(\"LI comment \"), F.col(\"l_orderkey\").cast(\"string\"), F.lit(\"-\"), F.col(\"l_linenumber\").cast(\"string\")))\n",
        "    .drop(\"o_orderdate\")\n",
        ")\n",
        "\n",
        "write_table(df_lineitem, \"lineitem\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 12 — Verify Row Counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tables = [\"region\", \"nation\", \"supplier\", \"part\", \"partsupp\", \"customer\", \"orders\", \"lineitem\"]\n",
        "\n",
        "print(f\"{'Table':<12} {'Row Count':>15}\")\n",
        "print(\"-\" * 30)\n",
        "for t in tables:\n",
        "    cnt = spark.table(f\"{full_schema}.{t}\").count()\n",
        "    print(f\"{t:<12} {cnt:>15,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 13 — Quick Preview: LINEITEM sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(spark.table(f\"{full_schema}.lineitem\").limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 14 — Quick Preview: ORDERS sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(spark.table(f\"{full_schema}.orders\").limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 15 — Sanity Check: TPC-H Query 1 (Pricing Summary Report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classic TPC-H Q1 — validates that data is query-ready\n",
        "q1 = f\"\"\"\n",
        "SELECT\n",
        "    l_returnflag,\n",
        "    l_linestatus,\n",
        "    SUM(l_quantity)                                       AS sum_qty,\n",
        "    SUM(l_extendedprice)                                  AS sum_base_price,\n",
        "    SUM(l_extendedprice * (1 - l_discount))               AS sum_disc_price,\n",
        "    SUM(l_extendedprice * (1 - l_discount) * (1 + l_tax)) AS sum_charge,\n",
        "    AVG(l_quantity)                                       AS avg_qty,\n",
        "    AVG(l_extendedprice)                                  AS avg_price,\n",
        "    AVG(l_discount)                                       AS avg_disc,\n",
        "    COUNT(*)                                              AS count_order\n",
        "FROM {full_schema}.lineitem\n",
        "WHERE l_shipdate <= DATE '1998-12-01' - INTERVAL 90 DAY\n",
        "GROUP BY l_returnflag, l_linestatus\n",
        "ORDER BY l_returnflag, l_linestatus\n",
        "\"\"\"\n",
        "\n",
        "display(spark.sql(q1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "634961d8",
      "metadata": {},
      "source": [
        "---\n",
        "All 8 TPC-H tables are now in `<catalog>.tpch` as managed Delta tables.\n",
        "\n",
        "To scale up, change `SCALE_FACTOR` in Cell 1 and re-run all cells.\n",
        "\n",
        "Continue with `01_bronze_layer.ipynb`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
