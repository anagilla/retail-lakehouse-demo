{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Medallion Architecture — Bronze Layer\n",
        "\n",
        "Ingests the raw TPC-H tables into a `retail_bronze` schema with audit columns\n",
        "(`_ingested_at`, `_source_table`, `_batch_id`), no business transformations applied.\n",
        "\n",
        "Liquid clustering is added on large tables for downstream read performance.\n",
        "\n",
        "**Prereq**: Run `00_generate_data.ipynb` first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1 — Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from datetime import datetime\n",
        "\n",
        "# ── Config ─────────────────────────────────────────────────────────────────────\n",
        "CATALOG       = spark.catalog.currentCatalog()\n",
        "SOURCE_SCHEMA = \"tpch\"           # where raw TPC-H tables live\n",
        "BRONZE_SCHEMA = \"retail_bronze\"  # target bronze schema\n",
        "\n",
        "source_fqn = f\"{CATALOG}.{SOURCE_SCHEMA}\"\n",
        "bronze_fqn = f\"{CATALOG}.{BRONZE_SCHEMA}\"\n",
        "\n",
        "# Batch metadata\n",
        "BATCH_ID     = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "INGESTED_AT  = F.current_timestamp()\n",
        "\n",
        "print(f\"Catalog       : {CATALOG}\")\n",
        "print(f\"Source schema : {source_fqn}\")\n",
        "print(f\"Bronze schema : {bronze_fqn}\")\n",
        "print(f\"Batch ID      : {BATCH_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2 — Create Bronze Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {bronze_fqn}\")\n",
        "spark.sql(f\"COMMENT ON SCHEMA {bronze_fqn} IS 'Bronze layer — raw ingestion of TPC-H retail data with audit columns'\")\n",
        "print(f\"Schema ready: {bronze_fqn}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3 — Generic Bronze Ingestion Function\n",
        "\n",
        "Every Bronze table gets three audit columns appended:\n",
        "| Column | Purpose |\n",
        "|--------|---------|\n",
        "| `_ingested_at` | Timestamp of when this row was loaded into Bronze |\n",
        "| `_source_table` | Fully qualified name of the upstream source |\n",
        "| `_batch_id` | Identifies this particular load run |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ingest_to_bronze(table_name, cluster_cols=None):\n",
        "    \"\"\"\n",
        "    Read from source, stamp audit columns, write to Bronze as managed Delta.\n",
        "    Optionally applies liquid clustering for large tables.\n",
        "    \"\"\"\n",
        "    src = f\"{source_fqn}.{table_name}\"\n",
        "    tgt = f\"{bronze_fqn}.{table_name}\"\n",
        "\n",
        "    df = (\n",
        "        spark.table(src)\n",
        "        .withColumn(\"_ingested_at\",  INGESTED_AT)\n",
        "        .withColumn(\"_source_table\", F.lit(src))\n",
        "        .withColumn(\"_batch_id\",     F.lit(BATCH_ID))\n",
        "    )\n",
        "\n",
        "    # Write as managed Delta table\n",
        "    df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tgt)\n",
        "\n",
        "    # Apply liquid clustering on large tables for query performance\n",
        "    if cluster_cols:\n",
        "        cols = \", \".join(cluster_cols)\n",
        "        spark.sql(f\"ALTER TABLE {tgt} CLUSTER BY ({cols})\")\n",
        "\n",
        "    cnt = spark.table(tgt).count()\n",
        "    print(f\"  ✓ {tgt:<50} {cnt:>12,} rows  (clustered by: {cluster_cols or 'none'})\")\n",
        "    return cnt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4 — Ingest All Tables to Bronze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Table → optional liquid clustering columns (pick high-cardinality filter/join keys)\n",
        "bronze_tables = {\n",
        "    \"region\":   None,\n",
        "    \"nation\":   None,\n",
        "    \"supplier\": [\"s_nationkey\"],\n",
        "    \"part\":     [\"p_brand\", \"p_type\"],\n",
        "    \"partsupp\": [\"ps_partkey\", \"ps_suppkey\"],\n",
        "    \"customer\": [\"c_nationkey\", \"c_mktsegment\"],\n",
        "    \"orders\":   [\"o_orderdate\", \"o_custkey\"],\n",
        "    \"lineitem\": [\"l_shipdate\", \"l_orderkey\"],\n",
        "}\n",
        "\n",
        "print(f\"{'Table':<50} {'Rows':>12}  Clustering\")\n",
        "print(\"=\" * 85)\n",
        "\n",
        "total = 0\n",
        "for tbl, cols in bronze_tables.items():\n",
        "    total += ingest_to_bronze(tbl, cluster_cols=cols)\n",
        "\n",
        "print(\"=\" * 85)\n",
        "print(f\"  Total rows in Bronze: {total:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5 — Add Table-Level Comments (Documentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "table_comments = {\n",
        "    \"region\":   \"Reference table: 5 world regions (TPC-H R_).\",\n",
        "    \"nation\":   \"Reference table: 25 nations mapped to regions (TPC-H N_).\",\n",
        "    \"supplier\": \"Dimension: suppliers with contact info and nation (TPC-H S_).\",\n",
        "    \"part\":     \"Dimension: product catalog with brand, type, size (TPC-H P_).\",\n",
        "    \"partsupp\": \"Bridge: part-supplier availability and cost (TPC-H PS_).\",\n",
        "    \"customer\": \"Dimension: customers with segment and nation (TPC-H C_).\",\n",
        "    \"orders\":   \"Fact: order headers with status, priority, dates (TPC-H O_).\",\n",
        "    \"lineitem\": \"Fact: order line items — largest table, grain of all analytics (TPC-H L_).\",\n",
        "}\n",
        "\n",
        "for tbl, comment in table_comments.items():\n",
        "    spark.sql(f\"COMMENT ON TABLE {bronze_fqn}.{tbl} IS '{comment}'\")\n",
        "\n",
        "print(\"Table comments applied.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6 — Validate: Row Counts & Schema Snapshot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick validation: compare source vs bronze row counts\n",
        "from pyspark.sql import Row\n",
        "\n",
        "validation = []\n",
        "for tbl in bronze_tables:\n",
        "    src_cnt = spark.table(f\"{source_fqn}.{tbl}\").count()\n",
        "    brz_cnt = spark.table(f\"{bronze_fqn}.{tbl}\").count()\n",
        "    match   = \"✓\" if src_cnt == brz_cnt else \"✗ MISMATCH\"\n",
        "    validation.append(Row(table=tbl, source_rows=src_cnt, bronze_rows=brz_cnt, status=match))\n",
        "\n",
        "df_val = spark.createDataFrame(validation)\n",
        "display(df_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7 — Demonstrate Delta Features: History & Time Travel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delta table history — shows every operation, useful for audit\n",
        "display(spark.sql(f\"DESCRIBE HISTORY {bronze_fqn}.lineitem\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delta detail — file count, size, clustering info\n",
        "display(spark.sql(f\"DESCRIBE DETAIL {bronze_fqn}.lineitem\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53788880",
      "metadata": {},
      "source": [
        "---\n",
        "All 8 tables are in `retail_bronze` with audit columns and liquid clustering.\n",
        "\n",
        "Continue with `02_silver_layer.ipynb`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
