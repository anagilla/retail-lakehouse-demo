{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI/ML Models — Customer Churn & Demand Forecasting\n",
        "\n",
        "Trains two models on the Gold layer using scikit-learn and Prophet, tracked via MLflow\n",
        "and registered in the Unity Catalog Model Registry.\n",
        "\n",
        "| Model | Algorithm | Purpose |\n",
        "|---|---|---|\n",
        "| Customer Churn Predictor | GradientBoosting (sklearn) | Identify at-risk customers |\n",
        "| Demand Forecaster | Prophet | Monthly revenue forecast per region |\n",
        "\n",
        "**Prereqs**: Run notebooks 00–05 first."
      ],
      "id": "b6d3932c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1 — Configuration & Installs"
      ],
      "id": "76ec448d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install scikit-learn mlflow prophet --quiet\n",
        "dbutils.library.restartPython()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "3f4d44fd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import mlflow\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "mlflow.set_registry_uri(\"databricks-uc\")\n",
        "\n",
        "CATALOG = spark.catalog.currentCatalog()\n",
        "GOLD    = f\"{CATALOG}.retail_gold\"\n",
        "SILVER  = f\"{CATALOG}.retail_silver\"\n",
        "MODELS  = f\"{CATALOG}.retail_models\"\n",
        "\n",
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {MODELS}\")\n",
        "spark.sql(f\"COMMENT ON SCHEMA {MODELS} IS 'ML models for retail analytics'\")\n",
        "\n",
        "print(f\"Catalog : {CATALOG}\")\n",
        "print(f\"Gold    : {GOLD}\")\n",
        "print(f\"Models  : {MODELS}\")\n",
        "print(f\"MLflow  : Unity Catalog\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7643fa74"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## MODEL 1: Customer Churn Prediction\n",
        "\n",
        "**Objective**: Predict which customers are likely to churn (become \"At Risk\", \"Needs Attention\", or \"Lost\") so marketing can intervene.\n",
        "\n",
        "**Algorithm**: scikit-learn GradientBoostingClassifier (serverless compatible — SparkML is blocked).\n",
        "\n",
        "**Label**: Binary — 1 = at-risk/lost, 0 = healthy."
      ],
      "id": "4244c5f2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2 — Prepare Training Data"
      ],
      "id": "3932b30a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load customer RFM data from Gold layer → pandas\n",
        "df_rfm = spark.table(f\"{GOLD}.gold_customer_rfm\")\n",
        "\n",
        "pdf = (\n",
        "    df_rfm\n",
        "    .withColumn(\"churn_label\",\n",
        "        F.when(F.col(\"rfm_segment\").isin(\"At Risk\", \"Needs Attention\", \"Lost\"), 1)\n",
        "         .otherwise(0)\n",
        "    )\n",
        "    .select(\n",
        "        \"customer_key\",\n",
        "        \"recency_days\",\n",
        "        \"frequency\",\n",
        "        F.col(\"monetary\").alias(\"lifetime_value\"),\n",
        "        \"avg_order_value\",\n",
        "        \"r_score\", \"f_score\", \"m_score\", \"rfm_score\",\n",
        "        \"customer_lifetime_days\",\n",
        "        \"market_segment\",\n",
        "        \"customer_region\",\n",
        "        \"churn_label\",\n",
        "    )\n",
        "    .toPandas()\n",
        ")\n",
        "\n",
        "print(f\"Total customers: {len(pdf):,}\")\n",
        "print(f\"\\nChurn distribution:\")\n",
        "print(pdf[\"churn_label\"].value_counts())"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "bb35cb5a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3 — Build scikit-learn Pipeline"
      ],
      "id": "999d48bc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, classification_report\n",
        "\n",
        "# Feature columns\n",
        "numeric_features = [\n",
        "    \"recency_days\", \"frequency\", \"lifetime_value\", \"avg_order_value\",\n",
        "    \"r_score\", \"f_score\", \"m_score\", \"rfm_score\", \"customer_lifetime_days\",\n",
        "]\n",
        "categorical_features = [\"market_segment\", \"customer_region\"]\n",
        "all_features = numeric_features + categorical_features\n",
        "\n",
        "X = pdf[all_features].copy()\n",
        "y = pdf[\"churn_label\"].values\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Train: {len(X_train):,} rows\")\n",
        "print(f\"Test:  {len(X_test):,} rows\")\n",
        "print(f\"Features: {len(all_features)} ({len(numeric_features)} numeric + {len(categorical_features)} categorical)\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6d849c43"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4 — Train & Evaluate with MLflow"
      ],
      "id": "64d994c2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build sklearn pipeline: encode categoricals + scale numerics + GBT\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", StandardScaler(), numeric_features),\n",
        "    (\"cat\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1), categorical_features),\n",
        "])\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"classifier\", GradientBoostingClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=5,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "    )),\n",
        "])\n",
        "\n",
        "# Train with MLflow tracking\n",
        "experiment_name = f\"/Users/{spark.sql('SELECT current_user()').collect()[0][0]}/retail_churn_experiment\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "with mlflow.start_run(run_name=\"churn_gbt_sklearn_v1\") as run:\n",
        "    mlflow.sklearn.autolog(log_models=False)  # auto-log params/metrics\n",
        "\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Metrics\n",
        "    auc = roc_auc_score(y_test, y_proba)\n",
        "    f1  = f1_score(y_test, y_pred)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    mlflow.log_metrics({\"test_auc\": auc, \"test_f1\": f1, \"test_accuracy\": acc})\n",
        "    mlflow.log_param(\"features\", str(all_features))\n",
        "    mlflow.log_param(\"algorithm\", \"GradientBoostingClassifier\")\n",
        "\n",
        "    # Log the model with signature\n",
        "    from mlflow.models.signature import infer_signature\n",
        "    signature = infer_signature(X_train, y_pred)\n",
        "    mlflow.sklearn.log_model(pipeline, \"model\", signature=signature, input_example=X_train.head(3))\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"  AUC-ROC  : {auc:.4f}\")\n",
        "    print(f\"  F1 Score : {f1:.4f}\")\n",
        "    print(f\"  Accuracy : {acc:.4f}\")\n",
        "    print(f\"  Run ID   : {run.info.run_id}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"\\n{classification_report(y_test, y_pred, target_names=['Healthy', 'Churn'])}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f1afb46c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5 — Register Model in Unity Catalog"
      ],
      "id": "4010e103"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_name = f\"{MODELS}.customer_churn_model\"\n",
        "\n",
        "model_uri = f\"runs:/{run.info.run_id}/model\"\n",
        "registered = mlflow.register_model(model_uri, model_name)\n",
        "\n",
        "print(f\"✓ Model registered: {model_name}\")\n",
        "print(f\"  Version: {registered.version}\")\n",
        "print(f\"  Source:  {registered.source}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9b695560"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set alias for production use\n",
        "from mlflow import MlflowClient\n",
        "client = MlflowClient()\n",
        "\n",
        "client.set_registered_model_alias(model_name, \"champion\", registered.version)\n",
        "print(f\"✓ Alias 'champion' set to version {registered.version}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d8581cef"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6 — Feature Importance & Explainability"
      ],
      "id": "09c40472"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract feature importances from the GBT model\n",
        "gbt_model = pipeline.named_steps[\"classifier\"]\n",
        "\n",
        "# Get feature names after preprocessing\n",
        "feature_names = numeric_features + categorical_features\n",
        "importances = gbt_model.feature_importances_\n",
        "\n",
        "fi_df = pd.DataFrame({\n",
        "    \"feature\": feature_names,\n",
        "    \"importance\": importances\n",
        "}).sort_values(\"importance\", ascending=False)\n",
        "\n",
        "print(\"Feature Importance:\")\n",
        "display(spark.createDataFrame(fi_df))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "22f0c493"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7 — Score All Customers (Batch Inference)"
      ],
      "id": "627da8bf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Score the full customer base\n",
        "X_all = pdf[all_features].copy()\n",
        "pdf[\"churn_prediction\"] = pipeline.predict(X_all)\n",
        "pdf[\"churn_probability\"] = np.round(pipeline.predict_proba(X_all)[:, 1], 4)\n",
        "\n",
        "pdf[\"risk_tier\"] = pd.cut(\n",
        "    pdf[\"churn_probability\"],\n",
        "    bins=[-0.01, 0.4, 0.6, 0.8, 1.01],\n",
        "    labels=[\"Low\", \"Medium\", \"High\", \"Critical\"]\n",
        ")\n",
        "\n",
        "# Select columns for Gold table\n",
        "churn_scores_pdf = pdf[[\n",
        "    \"customer_key\", \"market_segment\", \"customer_region\",\n",
        "    \"recency_days\", \"frequency\", \"lifetime_value\", \"rfm_score\",\n",
        "    \"churn_label\", \"churn_prediction\", \"churn_probability\", \"risk_tier\"\n",
        "]].copy()\n",
        "\n",
        "# Convert to Spark and save to Gold layer\n",
        "churn_scores_sdf = spark.createDataFrame(churn_scores_pdf.astype({\"risk_tier\": str}))\n",
        "churn_scores_sdf.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{GOLD}.gold_churn_scores\")\n",
        "\n",
        "print(f\"✓ Churn scores saved to {GOLD}.gold_churn_scores\")\n",
        "print(f\"  Total customers scored: {len(churn_scores_pdf):,}\")\n",
        "print(f\"\\nRisk tier distribution:\")\n",
        "display(\n",
        "    spark.table(f\"{GOLD}.gold_churn_scores\")\n",
        "    .groupBy(\"risk_tier\")\n",
        "    .agg(\n",
        "        F.count(\"*\").alias(\"customers\"),\n",
        "        F.round(F.avg(\"churn_probability\"), 3).alias(\"avg_churn_prob\"),\n",
        "        F.round(F.avg(\"lifetime_value\"), 0).alias(\"avg_ltv\"),\n",
        "    )\n",
        "    .orderBy(F.desc(\"avg_churn_prob\"))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "04cf0302"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## MODEL 2: Monthly Demand Forecast (per Region)\n",
        "\n",
        "**Objective**: Forecast monthly net revenue by region for the next 6 months.\n",
        "\n",
        "**Approach**: Use Prophet on each region's time series (pandas-based, serverless compatible)."
      ],
      "id": "020dcce8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8 — Prepare Monthly Revenue Data"
      ],
      "id": "18ecb82a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "monthly_pdf = (\n",
        "    spark.table(f\"{GOLD}.gold_monthly_sales\")\n",
        "    .groupBy(\"year_month\", \"region\")\n",
        "    .agg(F.sum(\"net_revenue\").alias(\"net_revenue\"))\n",
        "    .toPandas()\n",
        ")\n",
        "\n",
        "monthly_pdf[\"ds\"] = pd.to_datetime(monthly_pdf[\"year_month\"] + \"-01\")\n",
        "monthly_pdf = monthly_pdf.rename(columns={\"net_revenue\": \"y\"})\n",
        "monthly_pdf = monthly_pdf.sort_values([\"region\", \"ds\"])\n",
        "\n",
        "print(f\"Regions: {sorted(monthly_pdf['region'].unique())}\")\n",
        "print(f\"Date range: {monthly_pdf['ds'].min()} → {monthly_pdf['ds'].max()}\")\n",
        "print(f\"Total rows: {len(monthly_pdf):,}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "dc97a2f5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9 — Forecast per Region with Prophet"
      ],
      "id": "f3dd805b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from prophet import Prophet\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "all_forecasts = []\n",
        "\n",
        "for region in sorted(monthly_pdf[\"region\"].unique()):\n",
        "    region_df = monthly_pdf[monthly_pdf[\"region\"] == region][[\"ds\", \"y\"]].copy()\n",
        "\n",
        "    m = Prophet(\n",
        "        yearly_seasonality=True,\n",
        "        weekly_seasonality=False,\n",
        "        daily_seasonality=False,\n",
        "        changepoint_prior_scale=0.05,\n",
        "    )\n",
        "    m.fit(region_df)\n",
        "\n",
        "    future = m.make_future_dataframe(periods=6, freq=\"MS\")\n",
        "    forecast = m.predict(future)\n",
        "\n",
        "    result = forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]].merge(region_df, on=\"ds\", how=\"left\")\n",
        "    result[\"region\"] = region\n",
        "    result[\"is_forecast\"] = result[\"y\"].isna().map({True: \"forecast\", False: \"actual\"})\n",
        "    all_forecasts.append(result)\n",
        "\n",
        "    print(f\"  ✓ {region}: {len(region_df)} months history → 6 months forecast\")\n",
        "\n",
        "forecast_pdf = pd.concat(all_forecasts, ignore_index=True)\n",
        "\n",
        "# Save to Gold\n",
        "forecast_sdf = spark.createDataFrame(forecast_pdf[[\"region\", \"ds\", \"y\", \"yhat\", \"yhat_lower\", \"yhat_upper\", \"is_forecast\"]])\n",
        "forecast_sdf.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{GOLD}.gold_demand_forecast\")\n",
        "\n",
        "print(f\"\\n✓ Forecasts saved to {GOLD}.gold_demand_forecast\")\n",
        "display(\n",
        "    spark.table(f\"{GOLD}.gold_demand_forecast\")\n",
        "    .filter(F.col(\"is_forecast\") == \"forecast\")\n",
        "    .orderBy(\"region\", \"ds\")\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ae289dd7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10 — Log Forecast to MLflow"
      ],
      "id": "d2d0a410"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "experiment_name = f\"/Users/{spark.sql('SELECT current_user()').collect()[0][0]}/retail_demand_forecast\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "with mlflow.start_run(run_name=\"demand_forecast_prophet_v1\") as run:\n",
        "    mlflow.log_table(forecast_pdf, artifact_file=\"forecast_results.json\")\n",
        "\n",
        "    # MAPE on actuals\n",
        "    actuals = forecast_pdf[forecast_pdf[\"is_forecast\"] == \"actual\"].copy()\n",
        "    actuals[\"ape\"] = abs(actuals[\"y\"] - actuals[\"yhat\"]) / actuals[\"y\"]\n",
        "    mape = actuals[\"ape\"].mean() * 100\n",
        "\n",
        "    mlflow.log_metric(\"mape_pct\", round(mape, 2))\n",
        "    mlflow.log_param(\"forecast_horizon_months\", 6)\n",
        "    mlflow.log_param(\"regions\", str(sorted(forecast_pdf[\"region\"].unique().tolist())))\n",
        "\n",
        "    print(f\"  MAPE: {mape:.2f}%\")\n",
        "    print(f\"  Run ID: {run.info.run_id}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "40e6ab5d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11 — ML Summary"
      ],
      "id": "2d33b8a9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Churn model  : {MODELS}.customer_churn_model\")\n",
        "print(f\"  Scores     : {GOLD}.gold_churn_scores  ({spark.table(f'{GOLD}.gold_churn_scores').count():,} rows)\")\n",
        "print(f\"Forecast     : {GOLD}.gold_demand_forecast  ({spark.table(f'{GOLD}.gold_demand_forecast').count():,} rows)\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d921cb6d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "Both models trained and registered. Continue with `07_ai_agents.ipynb`."
      ],
      "id": "1aabf51b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}