{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Medallion Architecture — Silver Layer\n",
        "\n",
        "Cleanses, conforms, and enriches Bronze data into dimension and fact tables:\n",
        "\n",
        "| Silver Table | Source Tables | Purpose |\n",
        "|---|---|---|\n",
        "| `dim_geography` | region + nation | Denormalized geography hierarchy |\n",
        "| `dim_customer` | customer + nation + region | Enriched customer dimension |\n",
        "| `dim_supplier` | supplier + nation + region | Enriched supplier dimension |\n",
        "| `dim_part` | part | Cleansed product dimension |\n",
        "| `dim_date` | generated | Date dimension for analytics |\n",
        "| `fact_orders` | orders + customer | Enriched order headers |\n",
        "| `fact_lineitem` | lineitem + part + supplier + partsupp | Enriched line items with cost/revenue |\n",
        "\n",
        "Uses Delta CHECK constraints, liquid clustering, and column comments.\n",
        "\n",
        "**Prereq**: Run `01_bronze_layer.ipynb` first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1 — Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "CATALOG       = spark.catalog.currentCatalog()\n",
        "BRONZE_SCHEMA = \"retail_bronze\"\n",
        "SILVER_SCHEMA = \"retail_silver\"\n",
        "\n",
        "bronze = f\"{CATALOG}.{BRONZE_SCHEMA}\"\n",
        "silver = f\"{CATALOG}.{SILVER_SCHEMA}\"\n",
        "\n",
        "print(f\"Catalog : {CATALOG}\")\n",
        "print(f\"Bronze  : {bronze}\")\n",
        "print(f\"Silver  : {silver}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2 — Create Silver Schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {silver}\")\n",
        "spark.sql(f\"COMMENT ON SCHEMA {silver} IS 'Silver layer — cleansed, enriched retail dimensions and facts'\")\n",
        "print(f\"Schema ready: {silver}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3 — Helper: Write & Validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def write_silver(df, table_name, cluster_cols=None, comment=None):\n",
        "    fqn = f\"{silver}.{table_name}\"\n",
        "    df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(fqn)\n",
        "    if cluster_cols:\n",
        "        spark.sql(f\"ALTER TABLE {fqn} CLUSTER BY ({', '.join(cluster_cols)})\")\n",
        "    if comment:\n",
        "        spark.sql(f\"COMMENT ON TABLE {fqn} IS '{comment}'\")\n",
        "    cnt = spark.table(fqn).count()\n",
        "    print(f\"  ✓ {fqn:<55} {cnt:>12,} rows\")\n",
        "    return cnt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4 — dim_geography\n",
        "Denormalize region + nation into a single geography hierarchy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_region = spark.table(f\"{bronze}.region\").select(\"r_regionkey\", \"r_name\")\n",
        "df_nation = spark.table(f\"{bronze}.nation\").select(\"n_nationkey\", \"n_name\", \"n_regionkey\")\n",
        "\n",
        "dim_geography = (\n",
        "    df_nation\n",
        "    .join(df_region, df_nation.n_regionkey == df_region.r_regionkey, \"left\")\n",
        "    .select(\n",
        "        F.col(\"n_nationkey\").alias(\"nation_key\"),\n",
        "        F.col(\"n_name\").alias(\"nation_name\"),\n",
        "        F.col(\"r_regionkey\").alias(\"region_key\"),\n",
        "        F.col(\"r_name\").alias(\"region_name\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "write_silver(dim_geography, \"dim_geography\",\n",
        "    comment=\"Denormalized geography: nation → region hierarchy\")\n",
        "display(dim_geography)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5 — dim_customer\n",
        "Enrich customer with nation/region names and cleanse fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_cust = spark.table(f\"{bronze}.customer\")\n",
        "df_geo  = spark.table(f\"{silver}.dim_geography\")\n",
        "\n",
        "dim_customer = (\n",
        "    df_cust\n",
        "    .join(df_geo, df_cust.c_nationkey == df_geo.nation_key, \"left\")\n",
        "    .select(\n",
        "        F.col(\"c_custkey\").alias(\"customer_key\"),\n",
        "        F.trim(F.col(\"c_name\")).alias(\"customer_name\"),\n",
        "        F.trim(F.col(\"c_address\")).alias(\"address\"),\n",
        "        F.col(\"c_phone\").alias(\"phone\"),\n",
        "        F.col(\"c_acctbal\").alias(\"account_balance\"),\n",
        "        F.col(\"c_mktsegment\").alias(\"market_segment\"),\n",
        "        F.col(\"nation_name\"),\n",
        "        F.col(\"region_name\"),\n",
        "        # Derived: balance tier\n",
        "        F.when(F.col(\"c_acctbal\") < 0, \"Negative\")\n",
        "         .when(F.col(\"c_acctbal\") < 3000, \"Low\")\n",
        "         .when(F.col(\"c_acctbal\") < 7000, \"Medium\")\n",
        "         .otherwise(\"High\")\n",
        "         .alias(\"balance_tier\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "write_silver(dim_customer, \"dim_customer\",\n",
        "    cluster_cols=[\"market_segment\", \"region_name\"],\n",
        "    comment=\"Enriched customer dimension with geography and balance tier\")\n",
        "display(dim_customer.limit(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6 — dim_supplier\n",
        "Enrich supplier with geography."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_supp = spark.table(f\"{bronze}.supplier\")\n",
        "\n",
        "dim_supplier = (\n",
        "    df_supp\n",
        "    .join(df_geo, df_supp.s_nationkey == df_geo.nation_key, \"left\")\n",
        "    .select(\n",
        "        F.col(\"s_suppkey\").alias(\"supplier_key\"),\n",
        "        F.trim(F.col(\"s_name\")).alias(\"supplier_name\"),\n",
        "        F.trim(F.col(\"s_address\")).alias(\"address\"),\n",
        "        F.col(\"s_phone\").alias(\"phone\"),\n",
        "        F.col(\"s_acctbal\").alias(\"account_balance\"),\n",
        "        F.col(\"nation_name\"),\n",
        "        F.col(\"region_name\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "write_silver(dim_supplier, \"dim_supplier\",\n",
        "    cluster_cols=[\"region_name\"],\n",
        "    comment=\"Enriched supplier dimension with geography\")\n",
        "display(dim_supplier.limit(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7 — dim_part\n",
        "Cleanse and enrich product catalog."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_part = spark.table(f\"{bronze}.part\")\n",
        "\n",
        "dim_part = (\n",
        "    df_part\n",
        "    .select(\n",
        "        F.col(\"p_partkey\").alias(\"part_key\"),\n",
        "        F.trim(F.col(\"p_name\")).alias(\"part_name\"),\n",
        "        F.col(\"p_mfgr\").alias(\"manufacturer\"),\n",
        "        F.col(\"p_brand\").alias(\"brand\"),\n",
        "        F.col(\"p_type\").alias(\"part_type\"),\n",
        "        F.col(\"p_size\").alias(\"size\"),\n",
        "        F.col(\"p_container\").alias(\"container\"),\n",
        "        F.col(\"p_retailprice\").alias(\"retail_price\"),\n",
        "        # Derived: price band\n",
        "        F.when(F.col(\"p_retailprice\") < 950, \"Economy\")\n",
        "         .when(F.col(\"p_retailprice\") < 1050, \"Standard\")\n",
        "         .when(F.col(\"p_retailprice\") < 1500, \"Premium\")\n",
        "         .otherwise(\"Luxury\")\n",
        "         .alias(\"price_band\"),\n",
        "        # Derived: size category\n",
        "        F.when(F.col(\"p_size\") <= 10, \"Small\")\n",
        "         .when(F.col(\"p_size\") <= 30, \"Medium\")\n",
        "         .otherwise(\"Large\")\n",
        "         .alias(\"size_category\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "write_silver(dim_part, \"dim_part\",\n",
        "    cluster_cols=[\"brand\", \"part_type\"],\n",
        "    comment=\"Cleansed product dimension with price band and size category\")\n",
        "display(dim_part.limit(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8 — dim_date\n",
        "Generate a complete date dimension spanning the TPC-H date range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TPC-H dates: 1992-01-01 → 1998-12-31\n",
        "_start = F.to_date(F.lit(\"1992-01-01\"))\n",
        "_days  = 2557  # 7 full years\n",
        "\n",
        "dim_date = (\n",
        "    spark.range(0, _days)\n",
        "    .select(F.date_add(_start, F.col(\"id\").cast(IntegerType())).alias(\"date_key\"))\n",
        "    .withColumn(\"year\",         F.year(\"date_key\"))\n",
        "    .withColumn(\"quarter\",      F.quarter(\"date_key\"))\n",
        "    .withColumn(\"month\",        F.month(\"date_key\"))\n",
        "    .withColumn(\"month_name\",   F.date_format(\"date_key\", \"MMMM\"))\n",
        "    .withColumn(\"week_of_year\", F.weekofyear(\"date_key\"))\n",
        "    .withColumn(\"day_of_month\", F.dayofmonth(\"date_key\"))\n",
        "    .withColumn(\"day_of_week\",  F.dayofweek(\"date_key\"))\n",
        "    .withColumn(\"day_name\",     F.date_format(\"date_key\", \"EEEE\"))\n",
        "    .withColumn(\"is_weekend\",   F.dayofweek(\"date_key\").isin(1, 7))\n",
        "    .withColumn(\"year_quarter\", F.concat(F.col(\"year\"), F.lit(\"-Q\"), F.col(\"quarter\")))\n",
        "    .withColumn(\"year_month\",   F.date_format(\"date_key\", \"yyyy-MM\"))\n",
        ")\n",
        "\n",
        "write_silver(dim_date, \"dim_date\",\n",
        "    comment=\"Date dimension spanning 1992-1998 for TPC-H analytics\")\n",
        "display(dim_date.limit(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9 — fact_orders\n",
        "Enriched order headers joined with customer info."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_orders = spark.table(f\"{bronze}.orders\")\n",
        "df_cust_s = spark.table(f\"{silver}.dim_customer\").select(\n",
        "    \"customer_key\", \"market_segment\", \"nation_name\", \"region_name\", \"balance_tier\"\n",
        ")\n",
        "\n",
        "fact_orders = (\n",
        "    df_orders\n",
        "    .join(df_cust_s, df_orders.o_custkey == df_cust_s.customer_key, \"left\")\n",
        "    .select(\n",
        "        F.col(\"o_orderkey\").alias(\"order_key\"),\n",
        "        F.col(\"customer_key\"),\n",
        "        F.col(\"o_orderstatus\").alias(\"order_status\"),\n",
        "        F.col(\"o_totalprice\").alias(\"total_price\"),\n",
        "        F.col(\"o_orderdate\").alias(\"order_date\"),\n",
        "        F.col(\"o_orderpriority\").alias(\"order_priority\"),\n",
        "        F.col(\"o_clerk\").alias(\"clerk\"),\n",
        "        F.col(\"market_segment\"),\n",
        "        F.col(\"nation_name\").alias(\"customer_nation\"),\n",
        "        F.col(\"region_name\").alias(\"customer_region\"),\n",
        "        F.col(\"balance_tier\").alias(\"customer_balance_tier\"),\n",
        "        # Derived\n",
        "        F.year(\"o_orderdate\").alias(\"order_year\"),\n",
        "        F.quarter(\"o_orderdate\").alias(\"order_quarter\"),\n",
        "        F.month(\"o_orderdate\").alias(\"order_month\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "write_silver(fact_orders, \"fact_orders\",\n",
        "    cluster_cols=[\"order_date\", \"customer_key\"],\n",
        "    comment=\"Enriched order fact with customer attributes and date parts\")\n",
        "display(fact_orders.limit(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10 — fact_lineitem (largest Silver table)\n",
        "Fully enriched line items with product, supplier, and computed revenue/cost columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_li   = spark.table(f\"{bronze}.lineitem\")\n",
        "df_ps   = spark.table(f\"{bronze}.partsupp\").select(\"ps_partkey\", \"ps_suppkey\", \"ps_supplycost\")\n",
        "df_pt   = spark.table(f\"{silver}.dim_part\").select(\"part_key\", \"brand\", \"part_type\", \"price_band\", \"manufacturer\")\n",
        "df_sp   = spark.table(f\"{silver}.dim_supplier\").select(\"supplier_key\", \"supplier_name\", F.col(\"nation_name\").alias(\"supplier_nation\"), F.col(\"region_name\").alias(\"supplier_region\"))\n",
        "\n",
        "fact_lineitem = (\n",
        "    df_li\n",
        "    # Join partsupp for supply cost\n",
        "    .join(df_ps,\n",
        "          (df_li.l_partkey == df_ps.ps_partkey) & (df_li.l_suppkey == df_ps.ps_suppkey),\n",
        "          \"left\")\n",
        "    # Join part dimension\n",
        "    .join(df_pt, df_li.l_partkey == df_pt.part_key, \"left\")\n",
        "    # Join supplier dimension\n",
        "    .join(df_sp, df_li.l_suppkey == df_sp.supplier_key, \"left\")\n",
        "    .select(\n",
        "        # Keys\n",
        "        F.col(\"l_orderkey\").alias(\"order_key\"),\n",
        "        F.col(\"l_linenumber\").alias(\"line_number\"),\n",
        "        F.col(\"l_partkey\").alias(\"part_key\"),\n",
        "        F.col(\"l_suppkey\").alias(\"supplier_key\"),\n",
        "        # Measures\n",
        "        F.col(\"l_quantity\").alias(\"quantity\"),\n",
        "        F.col(\"l_extendedprice\").alias(\"extended_price\"),\n",
        "        F.col(\"l_discount\").alias(\"discount\"),\n",
        "        F.col(\"l_tax\").alias(\"tax\"),\n",
        "        # Computed revenue & cost\n",
        "        F.round(F.col(\"l_extendedprice\") * (1 - F.col(\"l_discount\")), 2).alias(\"net_revenue\"),\n",
        "        F.round(F.col(\"l_extendedprice\") * (1 - F.col(\"l_discount\")) * (1 + F.col(\"l_tax\")), 2).alias(\"gross_revenue\"),\n",
        "        F.round(F.col(\"ps_supplycost\") * F.col(\"l_quantity\"), 2).alias(\"supply_cost\"),\n",
        "        F.round(\n",
        "            F.col(\"l_extendedprice\") * (1 - F.col(\"l_discount\")) - F.coalesce(F.col(\"ps_supplycost\"), F.lit(0)) * F.col(\"l_quantity\"),\n",
        "            2\n",
        "        ).alias(\"profit\"),\n",
        "        # Status\n",
        "        F.col(\"l_returnflag\").alias(\"return_flag\"),\n",
        "        F.col(\"l_linestatus\").alias(\"line_status\"),\n",
        "        # Dates\n",
        "        F.col(\"l_shipdate\").alias(\"ship_date\"),\n",
        "        F.col(\"l_commitdate\").alias(\"commit_date\"),\n",
        "        F.col(\"l_receiptdate\").alias(\"receipt_date\"),\n",
        "        F.datediff(F.col(\"l_receiptdate\"), F.col(\"l_commitdate\")).alias(\"delivery_delay_days\"),\n",
        "        F.datediff(F.col(\"l_shipdate\"), F.col(\"l_commitdate\")).alias(\"ship_delay_days\"),\n",
        "        # Shipping\n",
        "        F.col(\"l_shipinstruct\").alias(\"ship_instruct\"),\n",
        "        F.col(\"l_shipmode\").alias(\"ship_mode\"),\n",
        "        # Enriched from dimensions\n",
        "        F.col(\"brand\"),\n",
        "        F.col(\"part_type\"),\n",
        "        F.col(\"price_band\"),\n",
        "        F.col(\"manufacturer\"),\n",
        "        F.col(\"supplier_name\"),\n",
        "        F.col(\"supplier_nation\"),\n",
        "        F.col(\"supplier_region\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "write_silver(fact_lineitem, \"fact_lineitem\",\n",
        "    cluster_cols=[\"ship_date\", \"order_key\"],\n",
        "    comment=\"Fully enriched line item fact with revenue, cost, profit, and dimension attributes\")\n",
        "display(fact_lineitem.limit(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11 — Data Quality Constraints\n",
        "Add Delta CHECK constraints on key Silver tables — Databricks enforces these on every future write."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "constraints = [\n",
        "    (\"dim_customer\", \"valid_customer_key\",  \"customer_key > 0\"),\n",
        "    (\"dim_supplier\", \"valid_supplier_key\",  \"supplier_key > 0\"),\n",
        "    (\"dim_part\",     \"valid_part_key\",      \"part_key > 0\"),\n",
        "    (\"dim_part\",     \"positive_price\",      \"retail_price > 0\"),\n",
        "    (\"fact_orders\",  \"valid_order_key\",     \"order_key > 0\"),\n",
        "    (\"fact_orders\",  \"positive_total\",      \"total_price > 0\"),\n",
        "    (\"fact_lineitem\",\"valid_quantity\",       \"quantity > 0\"),\n",
        "    (\"fact_lineitem\",\"valid_net_revenue\",    \"net_revenue >= 0\"),\n",
        "]\n",
        "\n",
        "for tbl, name, expr in constraints:\n",
        "    try:\n",
        "        spark.sql(f\"ALTER TABLE {silver}.{tbl} ADD CONSTRAINT {name} CHECK ({expr})\")\n",
        "        print(f\"  ✓ {tbl}.{name}\")\n",
        "    except Exception as e:\n",
        "        if \"already exists\" in str(e).lower() or \"CONSTRAINT_ALREADY_EXISTS\" in str(e):\n",
        "            print(f\"  ○ {tbl}.{name} (already exists)\")\n",
        "        else:\n",
        "            print(f\"  ✗ {tbl}.{name}: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12 — Silver Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "silver_tables = [\"dim_geography\", \"dim_customer\", \"dim_supplier\", \"dim_part\", \"dim_date\",\n",
        "                 \"fact_orders\", \"fact_lineitem\"]\n",
        "\n",
        "print(f\"{'Silver Table':<25} {'Rows':>15}  {'Columns':>8}\")\n",
        "print(\"=\" * 55)\n",
        "for t in silver_tables:\n",
        "    df = spark.table(f\"{silver}.{t}\")\n",
        "    print(f\"{t:<25} {df.count():>15,}  {len(df.columns):>8}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "7 Silver tables ready — dimensions denormalized, facts enriched with pre-computed revenue/profit columns.\n",
        "\n",
        "Continue with `03_gold_layer.ipynb`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}