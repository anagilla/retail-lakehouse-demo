{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lakebase — Online Serving Layer\n",
        "\n",
        "Publishes Gold-layer data to [Lakebase](https://docs.databricks.com/en/database.html) (PostgreSQL-compatible)\n",
        "for low-latency app and API access, plus Feature Store and Vector Search registration.\n",
        "\n",
        "```\n",
        "Gold Delta Tables  ──►  Lakebase Online Tables  ──►  Databricks App / AI Agent\n",
        "                   ──►  Feature Serving Endpoint  ──►  ML Model Serving\n",
        "                   ──►  Vector Search Index       ──►  RAG / Product Search\n",
        "```\n",
        "\n",
        "**Prereqs**: Run notebooks 00–03 first. Unity Catalog and a SQL Warehouse required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1 — Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CATALOG = spark.catalog.currentCatalog()\n",
        "GOLD    = f\"{CATALOG}.retail_gold\"\n",
        "SILVER  = f\"{CATALOG}.retail_silver\"\n",
        "SERVING = f\"{CATALOG}.retail_serving\"  # schema for online/serving artifacts\n",
        "\n",
        "print(f\"Catalog  : {CATALOG}\")\n",
        "print(f\"Gold     : {GOLD}\")\n",
        "print(f\"Silver   : {SILVER}\")\n",
        "print(f\"Serving  : {SERVING}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SERVING}\")\n",
        "spark.sql(f\"COMMENT ON SCHEMA {SERVING} IS 'Serving layer — online tables, feature store, vector search for retail analytics'\")\n",
        "print(f\"Schema ready: {SERVING}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2 — Prepare Source Tables for Online Serving\n",
        "\n",
        "Online tables require a **primary key**. We'll create serving-ready views/tables in the serving schema with explicit PKs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2a — Customer RFM Lookup (key: customer_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6184ed8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Customer RFM — used by apps and agents for real-time customer lookup\n",
        "df_rfm = spark.table(f\"{GOLD}.gold_customer_rfm\").select(\n",
        "    \"customer_key\",\n",
        "    \"market_segment\",\n",
        "    \"customer_nation\",\n",
        "    \"customer_region\",\n",
        "    \"recency_days\",\n",
        "    \"frequency\",\n",
        "    F.col(\"monetary\").alias(\"lifetime_value\"),\n",
        "    \"avg_order_value\",\n",
        "    \"rfm_score\",\n",
        "    \"rfm_segment\",\n",
        "    \"customer_lifetime_days\",\n",
        ")\n",
        "\n",
        "tbl_rfm = f\"{SERVING}.customer_rfm_online\"\n",
        "df_rfm.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tbl_rfm)\n",
        "\n",
        "# Set primary key (required for feature store / online serving)\n",
        "spark.sql(f\"ALTER TABLE {tbl_rfm} ALTER COLUMN customer_key SET NOT NULL\")\n",
        "try:\n",
        "    spark.sql(f\"ALTER TABLE {tbl_rfm} ADD CONSTRAINT pk_customer PRIMARY KEY (customer_key)\")\n",
        "except Exception as e:\n",
        "    if \"already exists\" in str(e).lower():\n",
        "        pass  # constraint already in place\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "print(f\"✓ {tbl_rfm} — {spark.table(tbl_rfm).count():,} rows, PK: customer_key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2b — Product Performance Lookup (key: brand + part_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d0acd67",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Product performance — keyed by brand + part_type for app/agent lookup\n",
        "df_prod = (\n",
        "    spark.table(f\"{GOLD}.gold_product_performance\")\n",
        "    .withColumn(\"product_id\", F.concat_ws(\"|\", \"brand\", \"part_type\", \"manufacturer\", \"price_band\"))\n",
        "    .select(\n",
        "        \"product_id\",\n",
        "        \"brand\",\n",
        "        \"part_type\",\n",
        "        \"manufacturer\",\n",
        "        \"price_band\",\n",
        "        \"net_revenue\",\n",
        "        \"total_profit\",\n",
        "        \"profit_margin_pct\",\n",
        "        \"total_quantity_sold\",\n",
        "        \"return_rate_pct\",\n",
        "        \"avg_discount\",\n",
        "    )\n",
        ")\n",
        "\n",
        "tbl_prod = f\"{SERVING}.product_perf_online\"\n",
        "df_prod.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tbl_prod)\n",
        "\n",
        "spark.sql(f\"ALTER TABLE {tbl_prod} ALTER COLUMN product_id SET NOT NULL\")\n",
        "try:\n",
        "    spark.sql(f\"ALTER TABLE {tbl_prod} ADD CONSTRAINT pk_product PRIMARY KEY (product_id)\")\n",
        "except Exception as e:\n",
        "    if \"already exists\" in str(e).lower():\n",
        "        pass\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "print(f\"✓ {tbl_prod} — {spark.table(tbl_prod).count():,} rows, PK: product_id\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2c — Supplier Scorecard Lookup (key: supplier_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13da3470",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_supp = spark.table(f\"{GOLD}.gold_supplier_scorecard\").select(\n",
        "    \"supplier_key\",\n",
        "    \"supplier_name\",\n",
        "    \"supplier_nation\",\n",
        "    \"supplier_region\",\n",
        "    \"net_revenue\",\n",
        "    \"total_profit\",\n",
        "    \"profit_margin_pct\",\n",
        "    \"on_time_delivery_pct\",\n",
        "    \"avg_delivery_delay_days\",\n",
        "    \"return_rate_pct\",\n",
        "    \"total_line_items\",\n",
        ")\n",
        "\n",
        "tbl_supp = f\"{SERVING}.supplier_score_online\"\n",
        "df_supp.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tbl_supp)\n",
        "\n",
        "spark.sql(f\"ALTER TABLE {tbl_supp} ALTER COLUMN supplier_key SET NOT NULL\")\n",
        "try:\n",
        "    spark.sql(f\"ALTER TABLE {tbl_supp} ADD CONSTRAINT pk_supplier PRIMARY KEY (supplier_key)\")\n",
        "except Exception as e:\n",
        "    if \"already exists\" in str(e).lower():\n",
        "        pass\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "print(f\"✓ {tbl_supp} — {spark.table(tbl_supp).count():,} rows, PK: supplier_key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7ee5ce7",
      "metadata": {},
      "source": [
        "---\n",
        "## 3 — Publish to Lakebase (PostgreSQL-Compatible Database)\n",
        "\n",
        "Lakebase provides a **PostgreSQL-wire-compatible** database for low-latency, high-concurrency serving.\n",
        "\n",
        "We connect via `psycopg2`, create tables, and push Gold data directly into Lakebase.\n",
        "Any application, BI tool, or AI agent can then query it with a standard PostgreSQL driver.\n",
        "\n",
        "> **Connection**: Uses the Lakebase sandbox endpoint. Update `LAKEBASE_HOST` if your endpoint differs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a20b2c42",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install psycopg2-binary --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "118c3e82",
      "metadata": {},
      "outputs": [],
      "source": [
        "import psycopg2\n",
        "import pandas as pd\n",
        "from pyspark.sql import functions as F\n",
        "from databricks.sdk import WorkspaceClient\n",
        "\n",
        "# ── Lakebase Connection Config ──────────────────────────────────────────────\n",
        "# Lakebase uses OAuth access tokens for auth (NOT PATs).\n",
        "# We extract the OAuth token from the SDK, which handles the auth flow.\n",
        "\n",
        "# Update this to match your Lakebase sandbox endpoint (Catalog > Database Instances).\n",
        "LAKEBASE_HOST = \"your-endpoint.database.us-east-1.cloud.databricks.com\"\n",
        "LAKEBASE_DB   = \"databricks_postgres\"\n",
        "LAKEBASE_USER = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
        "LAKEBASE_PORT = 5432\n",
        "\n",
        "# ── Get OAuth Token ──────────────────────────────────────────────────────────\n",
        "# Try multiple methods to get a working token for Lakebase.\n",
        "\n",
        "token = None\n",
        "method = None\n",
        "\n",
        "# Method 1: SDK authenticate() — gets the actual bearer token (OAuth or PAT)\n",
        "try:\n",
        "    w = WorkspaceClient()\n",
        "    headers = w.config.authenticate()\n",
        "    auth_header = headers.get(\"Authorization\", \"\")\n",
        "    if auth_header.startswith(\"Bearer \"):\n",
        "        token = auth_header[7:]  # strip 'Bearer ' prefix\n",
        "        method = \"SDK OAuth bearer token\"\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Method 2: Notebook context API token\n",
        "if not token:\n",
        "    try:\n",
        "        token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
        "        method = \"Notebook context token\"\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Method 3: SDK config.token directly\n",
        "if not token:\n",
        "    try:\n",
        "        sdk_token = WorkspaceClient().config.token\n",
        "        if sdk_token:\n",
        "            token = sdk_token\n",
        "            method = \"SDK config token\"\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Method 4: Widget fallback\n",
        "if not token:\n",
        "    dbutils.widgets.text(\"lakebase_token\", \"\", \"Paste your Databricks OAuth/PAT token\")\n",
        "    token = dbutils.widgets.get(\"lakebase_token\")\n",
        "    if token:\n",
        "        method = \"Notebook widget\"\n",
        "\n",
        "if token:\n",
        "    print(f\"✓ Token loaded via: {method}\")\n",
        "    print(f\"  Token prefix: {token[:8]}...  (length: {len(token)})\")\n",
        "else:\n",
        "    print(\"✗ No token found.\")\n",
        "\n",
        "def get_pg_conn():\n",
        "    \"\"\"Return a psycopg2 connection to Lakebase.\"\"\"\n",
        "    return psycopg2.connect(\n",
        "        host=LAKEBASE_HOST,\n",
        "        port=LAKEBASE_PORT,\n",
        "        dbname=LAKEBASE_DB,\n",
        "        user=LAKEBASE_USER,\n",
        "        password=token,\n",
        "        sslmode=\"require\",\n",
        "    )\n",
        "\n",
        "# Test the connection\n",
        "if token:\n",
        "    try:\n",
        "        with get_pg_conn() as conn:\n",
        "            with conn.cursor() as cur:\n",
        "                cur.execute(\"SELECT version()\")\n",
        "                ver = cur.fetchone()[0]\n",
        "        print(f\"\\n✓ Connected to Lakebase!\")\n",
        "        print(f\"  Host     : {LAKEBASE_HOST}\")\n",
        "        print(f\"  Database : {LAKEBASE_DB}\")\n",
        "        print(f\"  User     : {LAKEBASE_USER}\")\n",
        "        print(f\"  Version  : {ver[:80]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ Connection failed with {method}: {e}\")\n",
        "        print(f\"\\n  Debugging info:\")\n",
        "        print(f\"  Token prefix : {token[:8]}...\")\n",
        "        print(f\"  Token length : {len(token)}\")\n",
        "        print(f\"  User         : {LAKEBASE_USER}\")\n",
        "        print(f\"\\n  Next steps:\")\n",
        "        print(f\"  1. Try connecting via psql from your local machine to confirm creds work\")\n",
        "        print(f\"  2. Check if your user has been granted access to the Lakebase database\")\n",
        "        print(f\"  3. In Lakebase UI, check Database Settings → Allowed Users\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13dc47ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Push Gold tables into Lakebase ──────────────────────────────────────────\n",
        "# Read from serving Delta tables (with PKs) and write into Lakebase PostgreSQL.\n",
        "# Uses psycopg2.extras.execute_values for fast bulk inserts (20-50x faster).\n",
        "\n",
        "from psycopg2.extras import execute_values\n",
        "import time\n",
        "\n",
        "def push_to_lakebase(spark_table_fqn, pg_table_name, pk_col, create_sql):\n",
        "    \"\"\"\n",
        "    Read a Spark table, push it into a Lakebase PostgreSQL table.\n",
        "    Uses execute_values for fast batch inserts (~1000 rows per round-trip).\n",
        "    \"\"\"\n",
        "    t0 = time.time()\n",
        "    pdf = spark.table(spark_table_fqn).toPandas()\n",
        "    t_read = time.time() - t0\n",
        "\n",
        "    with get_pg_conn() as conn:\n",
        "        with conn.cursor() as cur:\n",
        "            # Drop and recreate (idempotent full refresh)\n",
        "            cur.execute(f\"DROP TABLE IF EXISTS {pg_table_name} CASCADE\")\n",
        "            cur.execute(create_sql)\n",
        "\n",
        "            # Fast bulk insert using execute_values (batches of 1000 rows)\n",
        "            cols = list(pdf.columns)\n",
        "            col_str = \", \".join(cols)\n",
        "            insert_sql = f\"INSERT INTO {pg_table_name} ({col_str}) VALUES %s\"\n",
        "            rows = [tuple(row) for row in pdf.itertuples(index=False, name=None)]\n",
        "\n",
        "            t1 = time.time()\n",
        "            execute_values(cur, insert_sql, rows, page_size=1000)\n",
        "            t_insert = time.time() - t1\n",
        "\n",
        "        conn.commit()\n",
        "\n",
        "    print(f\"  ✓ {pg_table_name:<30} {len(pdf):>10,} rows  (read: {t_read:.1f}s, insert: {t_insert:.1f}s)\")\n",
        "    return len(pdf)\n",
        "\n",
        "print(\"Push function ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "914e060a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Re-establish variables in case earlier cells were skipped after pip restart\n",
        "CATALOG = spark.catalog.currentCatalog()\n",
        "GOLD    = f\"{CATALOG}.retail_gold\"\n",
        "SILVER  = f\"{CATALOG}.retail_silver\"\n",
        "SERVING = f\"{CATALOG}.retail_serving\"\n",
        "\n",
        "# ── Table 1: Customer RFM ───────────────────────────────────────────────────\n",
        "push_to_lakebase(\n",
        "    spark_table_fqn=f\"{SERVING}.customer_rfm_online\",\n",
        "    pg_table_name=\"customer_rfm\",\n",
        "    pk_col=\"customer_key\",\n",
        "    create_sql=\"\"\"\n",
        "        CREATE TABLE customer_rfm (\n",
        "            customer_key       INTEGER PRIMARY KEY,\n",
        "            market_segment     VARCHAR(20),\n",
        "            customer_nation    VARCHAR(30),\n",
        "            customer_region    VARCHAR(20),\n",
        "            recency_days       INTEGER,\n",
        "            frequency          BIGINT,\n",
        "            lifetime_value     DOUBLE PRECISION,\n",
        "            avg_order_value    DOUBLE PRECISION,\n",
        "            rfm_score          INTEGER,\n",
        "            rfm_segment        VARCHAR(30),\n",
        "            customer_lifetime_days INTEGER\n",
        "        )\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "# ── Table 2: Product Performance ────────────────────────────────────────────\n",
        "push_to_lakebase(\n",
        "    spark_table_fqn=f\"{SERVING}.product_perf_online\",\n",
        "    pg_table_name=\"product_performance\",\n",
        "    pk_col=\"product_id\",\n",
        "    create_sql=\"\"\"\n",
        "        CREATE TABLE product_performance (\n",
        "            product_id           TEXT PRIMARY KEY,\n",
        "            brand                VARCHAR(20),\n",
        "            part_type            VARCHAR(80),\n",
        "            manufacturer         VARCHAR(30),\n",
        "            price_band           VARCHAR(20),\n",
        "            net_revenue          DOUBLE PRECISION,\n",
        "            total_profit         DOUBLE PRECISION,\n",
        "            profit_margin_pct    DOUBLE PRECISION,\n",
        "            total_quantity_sold  DOUBLE PRECISION,\n",
        "            return_rate_pct      DOUBLE PRECISION,\n",
        "            avg_discount         DOUBLE PRECISION\n",
        "        )\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "# ── Table 3: Supplier Scorecard ─────────────────────────────────────────────\n",
        "push_to_lakebase(\n",
        "    spark_table_fqn=f\"{SERVING}.supplier_score_online\",\n",
        "    pg_table_name=\"supplier_scorecard\",\n",
        "    pk_col=\"supplier_key\",\n",
        "    create_sql=\"\"\"\n",
        "        CREATE TABLE supplier_scorecard (\n",
        "            supplier_key            INTEGER PRIMARY KEY,\n",
        "            supplier_name           VARCHAR(30),\n",
        "            supplier_nation         VARCHAR(30),\n",
        "            supplier_region         VARCHAR(20),\n",
        "            net_revenue             DOUBLE PRECISION,\n",
        "            total_profit            DOUBLE PRECISION,\n",
        "            profit_margin_pct       DOUBLE PRECISION,\n",
        "            on_time_delivery_pct    DOUBLE PRECISION,\n",
        "            avg_delivery_delay_days DOUBLE PRECISION,\n",
        "            return_rate_pct         DOUBLE PRECISION,\n",
        "            total_line_items        BIGINT\n",
        "        )\n",
        "    \"\"\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4140a51d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Verify: Query Lakebase directly ─────────────────────────────────────────\n",
        "print(\"Lakebase table verification:\\n\")\n",
        "\n",
        "verify_queries = [\n",
        "    (\"customer_rfm\",        \"SELECT COUNT(*) AS cnt FROM customer_rfm\"),\n",
        "    (\"product_performance\", \"SELECT COUNT(*) AS cnt FROM product_performance\"),\n",
        "    (\"supplier_scorecard\",  \"SELECT COUNT(*) AS cnt FROM supplier_scorecard\"),\n",
        "]\n",
        "\n",
        "with get_pg_conn() as conn:\n",
        "    with conn.cursor() as cur:\n",
        "        print(f\"  {'Table':<25} {'Rows':>10}\")\n",
        "        print(f\"  {'='*40}\")\n",
        "        for tbl, sql in verify_queries:\n",
        "            cur.execute(sql)\n",
        "            cnt = cur.fetchone()[0]\n",
        "            print(f\"  {tbl:<25} {cnt:>10,}\")\n",
        "\n",
        "        # Sample point lookup — the real value of Lakebase\n",
        "        print(f\"\\n  Sample point lookup (customer_key = 42):\")\n",
        "        cur.execute(\"SELECT customer_key, rfm_segment, lifetime_value, rfm_score FROM customer_rfm WHERE customer_key = 42\")\n",
        "        row = cur.fetchone()\n",
        "        if row:\n",
        "            print(f\"    Key: {row[0]}, Segment: {row[1]}, LTV: ${row[2]:,.2f}, RFM: {row[3]}\")\n",
        "        else:\n",
        "            # Try first available customer\n",
        "            cur.execute(\"SELECT customer_key, rfm_segment, lifetime_value, rfm_score FROM customer_rfm LIMIT 1\")\n",
        "            row = cur.fetchone()\n",
        "            print(f\"    Key: {row[0]}, Segment: {row[1]}, LTV: ${row[2]:,.2f}, RFM: {row[3]}\")\n",
        "\n",
        "print(f\"\\n✓ Lakebase serving layer ready!\")\n",
        "print(f\"  Connect from any app: psql 'postgresql://{LAKEBASE_USER}@{LAKEBASE_HOST}/{LAKEBASE_DB}?sslmode=require'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4 — Feature Engineering & Feature Store\n",
        "\n",
        "Register customer features in the **Databricks Feature Store** (Unity Catalog) for use in ML models and real-time serving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22d621d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install databricks-feature-engineering --quiet\n",
        "from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup\n",
        "\n",
        "fe = FeatureEngineeringClient()\n",
        "\n",
        "# The customer_rfm_online table is already a great feature table.\n",
        "# Register it as a Feature Table so ML models can look up features at inference time.\n",
        "\n",
        "rfm_table = f\"{SERVING}.customer_rfm_online\"\n",
        "\n",
        "try:\n",
        "    fe.create_table(\n",
        "        name=rfm_table,\n",
        "        primary_keys=[\"customer_key\"],\n",
        "        description=\"Customer RFM features: recency, frequency, monetary value, segment scores\",\n",
        "        df=spark.table(rfm_table),\n",
        "    )\n",
        "    print(f\"✓ Feature table created: {rfm_table}\")\n",
        "except Exception as e:\n",
        "    if \"already exists\" in str(e).lower():\n",
        "        print(f\"○ Feature table already exists: {rfm_table}\")\n",
        "        # Update the feature table data\n",
        "        fe.write_table(\n",
        "            name=rfm_table,\n",
        "            df=spark.table(rfm_table),\n",
        "            mode=\"overwrite\",\n",
        "        )\n",
        "        print(f\"  ✓ Feature data refreshed\")\n",
        "    else:\n",
        "        print(f\"⚠ Feature store: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5 — Vector Search Index (for Product Semantic Search)\n",
        "\n",
        "Create a vector search index on product names so the AI Agent can do **semantic product lookup** (e.g., \"find products similar to polished brass\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "272726bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "CATALOG = spark.catalog.currentCatalog()\n",
        "SILVER  = f\"{CATALOG}.retail_silver\"\n",
        "SERVING = f\"{CATALOG}.retail_serving\"\n",
        "\n",
        "# Prepare a product catalog table with a text column for embedding\n",
        "df_product_catalog = (\n",
        "    spark.table(f\"{SILVER}.dim_part\")\n",
        "    .select(\n",
        "        F.col(\"part_key\").alias(\"product_id\"),\n",
        "        \"part_name\",\n",
        "        \"brand\",\n",
        "        \"manufacturer\",\n",
        "        \"part_type\",\n",
        "        \"container\",\n",
        "        \"price_band\",\n",
        "        \"retail_price\",\n",
        "        # Concatenated text field for embedding\n",
        "        F.concat_ws(\" | \",\n",
        "            F.col(\"part_name\"),\n",
        "            F.col(\"brand\"),\n",
        "            F.col(\"manufacturer\"),\n",
        "            F.col(\"part_type\"),\n",
        "            F.col(\"container\"),\n",
        "            F.col(\"price_band\"),\n",
        "        ).alias(\"search_text\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "tbl_catalog = f\"{SERVING}.product_catalog\"\n",
        "df_product_catalog.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tbl_catalog)\n",
        "\n",
        "spark.sql(f\"ALTER TABLE {tbl_catalog} ALTER COLUMN product_id SET NOT NULL\")\n",
        "try:\n",
        "    spark.sql(f\"ALTER TABLE {tbl_catalog} ADD CONSTRAINT pk_prod_catalog PRIMARY KEY (product_id)\")\n",
        "except:\n",
        "    pass  # Already exists\n",
        "\n",
        "# Enable Change Data Feed (required for vector search sync)\n",
        "spark.sql(f\"ALTER TABLE {tbl_catalog} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
        "\n",
        "print(f\"✓ {tbl_catalog} — {spark.table(tbl_catalog).count():,} products ready for vector search\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from databricks.sdk import WorkspaceClient\n",
        "\n",
        "w = WorkspaceClient()\n",
        "\n",
        "VS_ENDPOINT = \"retail_vs_endpoint\"\n",
        "VS_INDEX    = f\"{SERVING}.product_catalog_index\"\n",
        "\n",
        "# Step 1: Create vector search endpoint (if not exists)\n",
        "try:\n",
        "    w.vector_search_endpoints.create_endpoint(name=VS_ENDPOINT, endpoint_type=\"STANDARD\")\n",
        "    print(f\"✓ Vector search endpoint created: {VS_ENDPOINT}\")\n",
        "except Exception as e:\n",
        "    if \"already exists\" in str(e).lower():\n",
        "        print(f\"○ Endpoint already exists: {VS_ENDPOINT}\")\n",
        "    else:\n",
        "        print(f\"⚠ Endpoint creation: {str(e)[:200]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Create vector search index with auto-embedding\n",
        "# Uses Databricks' built-in embedding model (databricks-gte-large-en)\n",
        "try:\n",
        "    w.vector_search_indexes.create_index(\n",
        "        name=VS_INDEX,\n",
        "        endpoint_name=VS_ENDPOINT,\n",
        "        primary_key=\"product_id\",\n",
        "        index_type=\"DELTA_SYNC\",\n",
        "        delta_sync_index_spec={\n",
        "            \"source_table\": tbl_catalog,\n",
        "            \"pipeline_type\": \"TRIGGERED\",\n",
        "            \"embedding_source_columns\": [\n",
        "                {\"name\": \"search_text\", \"embedding_model_endpoint_name\": \"databricks-gte-large-en\"}\n",
        "            ],\n",
        "        },\n",
        "    )\n",
        "    print(f\"✓ Vector index created: {VS_INDEX}\")\n",
        "    print(f\"  Embedding column: search_text\")\n",
        "    print(f\"  Model: databricks-gte-large-en\")\n",
        "    print(f\"  Sync mode: TRIGGERED (manual refresh)\")\n",
        "except Exception as e:\n",
        "    if \"already exists\" in str(e).lower():\n",
        "        print(f\"○ Index already exists: {VS_INDEX}\")\n",
        "    else:\n",
        "        print(f\"⚠ Index creation: {str(e)[:200]}\")\n",
        "        print(f\"  If vector search is not available, the AI Agent will use SQL-based search instead.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6 — Verify Serving Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "serving_tables = [\n",
        "    \"customer_rfm_online\",\n",
        "    \"product_perf_online\",\n",
        "    \"supplier_score_online\",\n",
        "    \"product_catalog\",\n",
        "]\n",
        "\n",
        "print(f\"{'Serving Table':<30} {'Rows':>12}  {'Columns':>8}\")\n",
        "print(\"=\" * 55)\n",
        "for t in serving_tables:\n",
        "    df = spark.table(f\"{SERVING}.{t}\")\n",
        "    print(f\"{t:<30} {df.count():>12,}  {len(df.columns):>8}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test: look up a specific customer\n",
        "sample_cust = spark.table(f\"{SERVING}.customer_rfm_online\").limit(1).collect()[0]\n",
        "print(f\"Sample customer lookup:\")\n",
        "print(f\"  Key:       {sample_cust['customer_key']}\")\n",
        "print(f\"  Segment:   {sample_cust['rfm_segment']}\")\n",
        "print(f\"  LTV:       ${sample_cust['lifetime_value']:,.2f}\")\n",
        "print(f\"  RFM Score: {sample_cust['rfm_score']}\")\n",
        "print(f\"  Region:    {sample_cust['customer_region']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1877a4a0",
      "metadata": {},
      "source": [
        "---\n",
        "Lakebase tables, Feature Store entries, and Vector Search index are all live.\n",
        "\n",
        "Continue with `06_ai_ml_models.ipynb`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
